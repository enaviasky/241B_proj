
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}

\ifCLASSINFOpdf

\else

\fi
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{High Speed Energy-Efficient Convolutional Neural Network with Approximate Computing}

\author{Keertana Settaluri,
        Emily Naviasky\\% <-this % stops a space
        ksettaluri6@berkeley.edu, enaviasky@berkeley.edu,\\
        Department of Electrical and Computer Engineering, UC Berkeley}

\markboth{Journal of EE241B ,~Vol.~$\pi$, No.~0, March~2016}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle


\begin{abstract}
Convolutional neural networks are used in many vision applications such as pattern recognition and image classification. The amount of computation they require however, is immense. Approximate computing is a technique that reduces accuracy in favor of power savings and increased performance. Because convNets are innately resilient to error, using approximate computing blocks in convNet hardware proves to be very beneficial. This paper assesses several approximate adder and multiplier configurations, and chooses the Lower-part-OR adder and Broken-Array Booth multiplier to implement for each neural operation. Using UC Berkeley's Caffe and Cadence tools, we propose a new hardware approximate convNet accelerator that predicts promising power and performance metrics, with minimal tradeoff in accuracy. 
\end{abstract}

\begin{IEEEkeywords}
Convolutional Neural Network, Approximate Computing, Energy Efficiency
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
	\indent Deep neural networks have gained immense popularity in the recent decade, primarily in complex learning applications such as computer vision, speech recognition, and natural language processing. However, as problems become more complex, the size and sheer amount of computation required for neural networks drastically increases. Understandably, a significant amount of time and research is being used to develop a more efficient, less power intensive, and faster implementation of neural networks. \\
	
	\indent In particular, architectural modifications have been, and are currently being explored as a possible attempt to dynamically reduce the amount of neurons and connections in a network. Convolutional neural networks (convNets), which are widely used in many computer vision applications such as pattern recognition and image classification, are an excellent example. They were specifically developed to deal with a large input dataset, which would require an infeasible number of neurons in a traditional neural net. Despite making immense strides in computational efficiency, convNet operations still range in the millions or greater.[1] \\
	
	\indent The push to develop better convNets is largely fueled by their potential use in the consumer market. Current smart phone applications, such as Google translate [2], utilize these networks through the use of a mobile processor's SIMD instructions. Including more efficient hardware in their design would enable power conscious devices such as these to utilize convNets yet still be energy efficient. Though low power neural network hardware accelerators have been developed, their speed usually comes at the expense of generalization, as in the case of Eyeriss[3]. \\
		
	\indent ConvNets have an innate resilience to imperfections in precision due to the nature of the noisy data which they are used to interpret, which makes them an ideal application for approximate computation[12]. This paper seeks to explore hardware specific approximate computing as an optimization approach for convNet applications that efficiently reduces the precision of multiply and accumulate operations in order to speed up a system and save power. Several works have investigated the use of approximate computing to trade unnecessary accuracy for power and speed in neural networks. This work seeks to investigate different implementations of approximate computing applied specifically to convNets. 

\section{Problem Description}
	\indent Using a convNet to classify a single, small image can be very computationally expensive. A typical convNet, such as AlexNet [1], uses 2.3 million weights, and requires 666 million MACs per 227x227 image. An even more intensive implementation, VGG16 [4], uses 14.7 million weights, and requires a staggering 15.3 billion MACs per 224x224 image.\\
	
	\indent Choosing to tradeoff unnecessary accuracy for power savings and faster operation is not a difficult design decision. However, this decision must also take into consideration area limitations. Because silicon is expensive, designing an entire block for approximate hardware would have to imply significant power savings to be worth the area. Thus it is beneficial to explore whether convNets are an appropriate application for approximate computing. This paper will begin by examining convNets and approximate computing in more detail.
	
\subsection{Convolutional Neural Nets}
	\indent In general, neural networks are a classification algorithm well suited for noisy data[12]. They are composed of a highly connected mesh of nodes, where each connection represents some weight. Each node adds the weighted combination of the nodes from the previous layer. If the sum of weights is above a certain threshold function, then the node is activated and contributes its weight to the next layer; whereas, an inactivate node has a weight of zero. In this way, neural nets can represent very complex functions. The more layers, nodes, and interconnections there are, the more complex the representable functions are. However, larger and more complex nets require significantly more training. The training of these neural nets is typically done using back propagation and depending on the correctness of an output, training revisits all of the activated interconnects and negatively/positively reinforces the incorrect/correct answer by changing the mesh's weights. \\
	
	\indent ConvNets use learnable weights and biases similar to regular neural networks, however the arrangement of neurons in three dimensions makes them uniquely suited for processing input images. Because each neuron in a convNet only connects to a local region of the input volume, as opposed a traditional neural network, where every neuron is connected to each other, convNets are ideal for dealing with matrix shaped data such as images.  A typical convNet architecture consists of a convolutional layer, pooling layer, and fully-connected layer. These layers compute the dot product between the weights and the local region where a neuron is located, downsampling of operations in the spatial dimension, and computing class scores, respectively. Furthermore, convNets also utilize an activation function between the convolutional and pooling layers in order to increase nonlinearity which thereby makes training faster. \\
	
	\indent ConvNets already implement approximate computing at a software level[2]. On a hardware level, most research into power savings and approximate computation of neural networks focuses on general neural nets as opposed to convNets, but have found a marked improvement in power and speed without sacrificing significant accuracy of the net. Implementations such as AxNN [6] show varying degrees of power savings for minimal loss in accuracy. AxNN uses back propagation--- an algorithm used to change weights when training neural nets--- to identify less important nodes and decrease their accuracy by shortening the bit-width used in computation. This paper claims to achieve up to 1.9X energy benefits for less than 1\% loss of net accuracy, and an even greater factor of 2.3X when 7.5\% loss is permissible at the output[6]. This result validates that approximate computing can result in significant power savings. However, there are many approximate computing algorithms besides reducing bit-width, that show even more promise for power to accuracy trade-off.

\subsection{Approximate Computing}

	\indent Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of an operation in order to speed up a system and save power. Approximate computing can occur at an operational or algorithmic level. Algorithmic level approximate computing has been thoroughly explored and used in applications that respond well to early termination when an answer is close enough, such as digital filters[7] and Support Vector Machines[8]. However, this paper will focus on approximate computing at an operational level.\\
	

	\subsubsection{Error Quantification}
	\indent 	There are several metrics for quantifying the error from approximate computing. Error rate (ER) is the probability of an error occurring and is common among many approximate algorithms[6]. Error significance (ES) determines how far the approximate answer is from its accurate counterpart. Error mean (EM), error mean square, hamming distance, error min/max, or some combination thereof, are typical methods of characterizing ES; choosing which metric to calculate largely depends on the application[8]. The following analysis of approximate operations will use error rate and error mean. For a neural net application, the thresholding function inside of each neuron eliminates some small constant error, but is unable to handle infrequent but large errors. In addition, because each neuron sums a very large number of weighted inputs, a small and frequent error will place the average error close to the mean. Therefore, a higher ER is acceptable for a neural net application, but ES must be smaller. \\	
	
	\subsubsection{Approximate Adders}
	\indent Many implementations of approximate adders exist. Voltage scaling, for example, is a technique that reduces supply voltage without scaling frequency, and introduces error to both adders and multipliers because timing constraints are not met. Voltage scaled approximation is an effective method of decreasing power but needs to be carefully designed to ensure that significant bits behave well under scaled voltage conditions without risking a large ES.[8] In a similar vein, Speculative Latency Adders[9] allow delay to dictate error; these adders rely on the fact that the probability of a worst-case carry chain is small and will terminate early. This results in increased performance but minimal power savings, especially because this method is usually paired with error recovery.[8] The rest of this section examines adders with logical as opposed to transistor level error, such as the Error Tolerant Adder[10][11] and Lower-part-OR adder[12], as they seem to indicate better efficiency metrics. \\
	
	\indent The Error Tolerant Adder I (ETA 1) divides the adder into subsections and cuts the carry propagation chain between each section[10]. Some of the error induced by ignoring carries is corrected in subsequent versions of the Error Tolerant Adders (ETA II/III) by taking just the upper bits of the previous subsection into account[11]. Akin to the Error Tolerant Adders, the Dithering Adder[13] cuts the carry chain, but alternates the value of the replacement carry-in after each addition. This causes the addition to be either an an over or under approximation and makes the Dithering Adder well suited for accumulation. The ER of Error Tolerant Adders is between 20-25\% (depending on various options) but no mention is made of EM. Although the ETA estimates a 50\% reduction in power consumption compared to a typical adder, presumably because the answer is calculated in half the time, other adders indicate better power efficiency.\\
	
	\indent The Lower-part-OR (LPO) takes removal of lower bit carries to an extreme[12]. The LPO adder defines some number of lower bits and uses OR instead of XOR. No carries are propagated in these lower bits; only a single logical AND is performed on the most significant bits of the lower-part to choose a carry-in for the upper part. The paper[12] does not explore power savings, but the lack of carry propagation in the lower bits suggests somewhat significant power savings compared to other approximate adders that propagate carries through the lower bits. The ER for this adder is extremely high, and the minimum and maximum errors increase with a longer lower-part length. It is interesting to note, however, that the EM is completely independent of the word length or the lower-part length. The probability and magnitude of error in all bits except the lowest is equal, and thus the EM only -0.25. The lower power and low EM of this adder make it particularly appropriate for neural nets, and the paper[12]  even tests this adder on a neural net for face recognition. They found that the neural net could classify well with all of its adders approximating the lowest 9 of a 13-bit number. These characteristics make LPO a very good choice for a convNet application.\\
	  
\subsubsection{Approximate Multipliers}
	\indent Most research in approximate multipliers extends from existing approximate adder implementations. Solely replacing every partial product with any version of an approximate adder, however, creates too much error. Error correction methods or different, multiplier specific implementations have been proven to be more effective. For example, a multiplier with configurable partial recovery error detection was created by using an input pre-processing approximate adder for all partial products. This adder exchanges input bits in the case where one input bit is a 0 and the other is a 1. This stacking provides a means to cut the carry propagation chain, predicting up to a 69\% savings in power and up to 20\% decrease in delay. Though this multiplier predicts near negligible ES compared to other approximate multipliers because of error detection logic, this additional sub-block creates a larger area overhead. [14]\\
	
	\indent Another implementation manipulates the partial product by splitting the input bits evenly into two different components. The most significant bits are left untouched, and regular partial products are calculated. Carry propagate is removed for the lower order bits, and every bit position from left to right (MSB to LSB) is checked to see if the product of any two bits is 1. If this partial product is 1, every bit after and to the right of this location is set to 1. Simulations in 0.18-$\mu$ technology estimate a 50-97\% savings in power [15], but only several cases were considered in determining this power metric. In addition, the ES is reported to be 94-100\% [15]. Again however, these values only included five different input patterns. It is safe to say, therefore, that actual implementation is unlikely to meet the reported power savings to error ratio. \\
	
	\indent A promising approach uses a Broken-Array Booth Multiplier and claims up to a 58\% savings in power. The multiplier works by defining a Vertical Breaking Level (VBL), where any partial products located to the right of this line are not calculated. Using a world length of 12, the mean error proposed by this paper ranges from -3.50 for a VBL of three to -789 for a VBL of nine. The VBL for the 58\% power reduction was set to 15 in a 16x16 multiplication, and the corresponding tradeoff in accuracy is minimal[16]. 
	
\section{Implementation}

\subsubsection{ASIC Implementation}
	\indent ASIC designs for both the approximate multiplier and adder (with varying bit widths) were implemented in the FDSOI 32nm CMOS process. Each design was compiled using DC and ICC, and power measurements were obtained using Synopsys PrimeTime. In order to obtain a comparable baseline, the clock periods for both the adder and multiplier blocks were fixed, and the obtained positive slack was used to estimate operating frequency. \\
	
	\indent Pertaining to the implementation of the LPO adder, DC synthesis was permitted to optimize the upper bits used for accurate addition by using the '+' operator. The lower bits used a logical OR as initially proposed by [12]. In addition to obtaining power, area, and performance metrics for different bit width approximations, an accurate adder was pushed through the flow as a point of reference. \\
	
	\indent Despite [16] proposing using a Booth-Encoded version of a multiplier, [17] proposes that a simple partial product generator with a ripple carry adder at the final stage would have the lowest power in a CMOS 0.18um technology. Making the assumption that this holds in 32nm CMOS technology, our implementation of the multiplier used a simple partial product generator instead of a booth-encoded version. However, the concept of a VBL was still used, and power, area, and performance metrics were obtained by varying the VBL. Again, an accurate simple partial product generated multiplier was pushed through flow as a reference. \\
			
	\indent Combining convNets and approximate computing has already shown power and performance improvements. In particular, AxNN[6] examined approximate computing in a neural net application, because they believed neural nets are uniquely suited for approximate computing, and saw almost 2X power improvement for less than 1\% loss in quality. Specifically, they made use of the fact that certain neurons in a net are less sensitive than others when determining the correct answer. They replaced these less sensitive nodes with approximate nodes. In addition, they took advantage of the fact that training a neural net naturally corrects for and decreases noise and error, by adding a short re-training period after introducing the approximate nodes. This allowed the net to correct for approximations that were introduced. Comparatively, AxNN found that replacing every node with approximate hardware resulted in a vertically shifted power versus accuracy loss curve when compared to only the less sensitive nodes being replaced[6]. This implies that in our work, we can apply approximate computing to all of the nodes regardless of their sensitivity to obtain a good benchmark. Afterwards, application of more careful neuron selection will result in further improvement.\\ 

	\indent AxNN imposed approximation by varying bit-width. Though bit-width approximation is a very power efficient technique for computing, it introduces a large mean error. The Lower-Part-OR Adder is somewhat more power expensive but introduces only the addition of one OR gate per lower bit. Its mean error, however, is significantly smaller. Replacing bit-width approximation with LPO adders should result in significantly higher accuracy, and thereby allow the net to be pushed to lower power consumption for the same error rate. Using similar inferences, we expect that the Broken-Array Booth Multiplier, which has the most promising power per accuracy metric compared to other multipliers, will function at lower power for equivalent error in the neural network as well.\\

	\indent We will implement Lower-Part-OR approximate adder as well as Broken-Array Booth multiplier in 32nm LP CMOS technology, and use Cadence simulations to calculate average power per operation and confirm mean error. We will also implement an accurate adder and multiplier as well as a  bit-width varying approximate adder to compare performance metrics.\\

	\indent Because convNets are error tolerant, it is difficult to judge their accuracy without physically testing data in the net, and correspondingly observing the change in accuracy with which the data is classified. In our implementation, we choose to utilize the small, practical, and well-established convolutional neural net architecture, LeNet[17], which was created in the 1980s to recognize handwritten digits. LeNet makes use of the, now typical, Stochastic Gradient Descent (SGD) algorithm and Graph Transformer Networks (GTN), and defined the modern framework for modern convNet architectures. Any power and performance metrics we obtain using LeNet, therefore, are still relevant and practical to present convNets. In addition, UC Berkeley has created a deep learning framework in Python called Caffe that implements a slightly modified version of LeNet. By adapting this architecture, we will artificially introduce error in neuron operations to match the approximation method from our simulated hardware. In addition, we will explore introducing error in different layers of the convolutional net, specifically to see if one, two, or all of the layers are equally error tolerant. By varying the accuracy of the adders and multipliers in each neuron and testing data in the net, we will develop a metric and determine the accuracy of the convNet as a whole. Using Python, the number of adds and multiplications performed will be compiled and combined with the power numbers from the hardware simulation to obtain an approximate power measurement. \\

	\indent We expect our convNet to be able to tolerate some percentage of error in each node's arithmetic, and are interested in seeing the relation between error of a neuron versus accuracy of the convNet as a whole. Despite the fact that most works do not focus on this relationship between neuron error and net accuracy, we feel that this is an interesting correlation to investigate, and would enable generalizations and rules-of-thumb to be made about arithmetic error tolerance in many neural networks.\\
	
\subsubsection{Error in the adder}
\indent Error in the adder comes from the loss of carries propagating through the lower bits and the OR rather than an XOR operation which result in an underestimation. However, due to the carry in from the MSBs of the approximated bits, it is possible to overestimate, even without the carry propagating through the lower bits. The mean of the LOR approximate adder is -.25, but as is apparent in figure XXX, it has a very non-gaussian spread. With a slight bias towards large positive errors and small but frequent negative errors. Thus, it is unlikely to get a mean as low as -.25 numbers without adding every possible $2^{32}$ bit number, but the error still reduces the more numbers that are approximately summed together, which makes this added well suited to CNNs.

\indent Because we are using particular data as opposed to randomly distributed numbers, the error introduced to the CNN from the adder is strongly dependent on the value of $K$ and the spread of the input data. If the number of approximated bits becomes larger than $K$, then the addition becomes simply OR-ing together a lot of bits which results in an accumulation of ones in the lower bits rather than an accumulation in the typical summing sense. \\

\subsubsection{Error in the multiplier}
<this might need to come after the explanation of K>
\indent The multiplier approximates by zeroing out lower bits rather than calculating them. This can result in mean and probability of error worse than our addition approximation. The error from approximation can be thought of as a floor rounding error. Because of this the multiplier error relies on the $K$ value significantly, since the lower bits are zeroed out and for smaller $K$ the value is lost. 
\indent After a multiplication we get a 64 bit answer, which has $K^2$ term to map it to floating point. Consider: $(A\times K) \times (B\times K) = (A\times B)K^2$. We can still think of the right half of that equation as a floating point approximation, but with a quantization step of $\frac{1}{K^2}$. We were expecting a $K$ mapping for the addition, so in order to remove the second $K$ and turn the answer back into a 32 bit number, we divide by $K$, which since we chose $K$ as a power of 2, is just a shift. This conversion back to a 32 bit number could conceivably be a problem in a different application, since upper bits might be lost if the division by K is only a shift by 27 instead of a full 32. However, in our application this is not a problem, because all weight values $w$ are $|w|<1$. This means that our multiplier is actually very well suited to our application because we are already disregarding the lower 32 bits and we would like to waste as few resources on them as possible, however, we still get the accuracy from the upper bits of the lower PPs that rounding the multiplicands earlier would not give us.<clear?>

\subsection{Caffe}
	\indent Caffe is a C++ framework for training and testing generalized CNNs. CNNs are composed of different layers which accept inputs, perform some specified operation such as a filter convolution, and create outputs. Collections of data (images of numbers in this case) are moved through the layers and are referred to as blobs<necessary? maybe not>. There is a python wrapper which we used in our implementation.
\subsubsection{Lenet}
	\indent Lenet is a specific CNN configuration of two convolutional layers, each followed by a pooling layer, and with a final fully connected layer. Lenet is traditionally used to classify handwritten numbers, and can achieve high accuracy in this specific task without requiring intense computing resources to train and run, which makes it idea for this project.
	\indent To measure the effect of 
PIKTURE
\subsubsection{Mapping Floating point to Integer}
	\indent Caffe uses 32 bit floating point operations. CNN usually normalize their data values between 0 and 1 to keep the floating point numbers small. This can improve the gradient decent operations<go check that wording, future emily> during training. However, this means that the net has to deal with floating point even during testing. It does depend on the processor, but generally, floating point operations are slower and more power hungry than integer operations. We assume that for our application, there is no approximation during testing. Since approximation occurs during testing only, and we have a good idea of the maximum values that we will see in our CNN, we can map from floating point to integer. Thus, we can take advantage of faster and lower power integer operations. We implemented a 32 x 32 bit integer approximate multiplier and a 32 bit approximate adder. We decided on a linear mapping of float to integer: $Integer=K*Float$ where $K$ is a power of 2 so that mapping can be implemented as shift. There are two sources of error from linearly mapping from floating point to integer. The first source of error is that floating point can represent extremely small values, but an integer mapping will have a quantization step size of $\frac{1}{K}$, so for a large K this will be small. The second error is that some input outside of our data set could conceivably overflow our integer mapping and cause a very large error. This an unlikely occurence, given that the mean and sigma of our data points are XXX(7.2?) and XXX, but the potential error is extremely large, so we gave ourselves buffer room and decided our upper range that we would represent was 16. Finally, we are doing signed 32 bit integer operations, so the largest number that we can represent is $2^{31}-1$, and we decided that the largest floating point that we want to represnt is 16. Therefore, we chose $K = 2^{28}$. We used this K value for our power caluculations and for our error approximation.
	
\subsubsection{Introducing error into the CNN}
The CNN has three layers that are addition and multiplication intensive: convolution layer 1 (conv1), convolution layer 2 (conv2), and the fully connnected layer (fc1). Due to the labrinthine nature of Caffe, it was outside of the scope of our project to implement an actual approximate adder and multiplier inside of the CNN code to judge the effect of approximation exactly. However, the python wrapper for Caffe is set up specifically to allow developers to examine and edit data in between layers.  We introduce an estimated percent error at each of these layers.

We obtain percent error individually for each layer, because each layer does a different number of operations on a different range of numbers. For example, to obtain error for the output of the conv2 layer we ran 10000 training images through the unapproximated CNN and collected 500000 conv2 weights and 500000 non-zero sample numbers from the output of the conv1 layer to serve as our expected input to the conv2 layer. (We ignored the output of the pool1 pooling layer, because all pooling does is throw away a fourth of the data.) Layer conv2 does a dot product between a weight matrix and a section of the image. Therefore, to obtain an approximation of multiplication error, we approximately multiplied a weight with a data point and measured percent error from the correct answer. To introduce that error into the CNN we randomly choose a percent error from the sample error that we just calculated for that layer and introduce it to the output of conv2 before it is passed on to the next layer.
The addition approximation error is obtained similarly, but it takes into account that there are multiple additions. Due to the nature of the LOR approximate adder, the mean error is very low, so it is an advantage that there are multiple approximated additions. For example, the conv2 layer does a dot product between 25 numbers, so those 24 additions are likely to result in a lower error overall. To account for this, addition percent error is calculated by approximately adding 25 numbers for both conv1 and conv2 and adding 80 for fc1. To make sure that the numbers are the correct size, the sample numbers which are added in this case are collected inputs to a layer multiplied by random weights of that layer. The percent error is obtained from these multiple additions and is introduced in between layers in the same way as the multiply.<might need to clarify again>

BIG IMAGE OF LAYERS WITH PERCENT ERROR INTERRUPTIONS


\section{Results}
	\indent 
%	Power, speed, area stuff
%		Mult
%		adder
%	CNN accuracy
	\indent We found that with careful selection of a $K$ value, CNN calculations can be significantly reduced in power, speed, and chip area without significantly reducing neural net accuracy.
%		Mult
%		Adder
%		both



\
\section{Conclusion}
	\indent ConvNets are a prevalent, power intensive applications, and based on examination of previous works, they are also an excellent application for approximate computation. Approximate computing has already shown improvements in power and speed, and implementing an LPO approximate adder and Broken-Array Booth Multiplier inside of neurons should result in an improvement over these factors. We will compare power using a Synopsys generated model of approximate adders and multipliers, and judge error resiliency of the approximate net using Caffe. We expect an improvement in average power versus accuracy of the network. \\

\vspace{63mm} 
\begin{thebibliography}{1}

\bibitem{}
 A. Krizhevsky, I. Sutskever, G. E. Hinton, "ImageNet Classification with Deep Convolutional Neural Networks," Neural Information Processing Systems, pp. 1097‚Äì1105, 2012.	
\bibitem{} Otavio Good, ‚ÄúHow Google Translate squeezes deep learning onto a phone,‚Äù Google Translate, July 29, 2015. http://googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html
\bibitem{} Y. Chen, T. Krishna , J. Emer, ‚ÄúEyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks,‚Äù ISSCC 2016.
\bibitem{} K. Simonyan, A. Zisserman, ‚ÄúVery Deep Convolutional Networks for Large- Scale Image Recognition,‚Äù CoRR, 2014. 
\bibitem{} Karpathy. A, "CS231n Convolutional Neural Networks for Visual Recognition," January, 2016. http://cs231n.github.io
\bibitem{} S. Venkataramani, ‚ÄúAxNN: Energy Efficient Neuromorphic System using Approximate Computing,‚Äù ISLPED 2014.
\bibitem{} K. Roy, A. Raghunathan, ‚ÄúApproximate Computing: An Energy-Efficient Computing Technique for Error Resilient Applications,‚Äù Computer Society Annual Symp. on VLSI, 2015.
\bibitem{} J. Han, M. Orshansky, ‚ÄúApproximate Computing: An Emerging Paradigm For Energy-Efficient Design,‚Äù ETS, 2013.
\bibitem{} S.-L. Lu, ‚ÄúSpeeding up processing with approximation circuits,‚Äù
Computer, vol. 37, no. 3, pp. 67-73, 2004.
\bibitem{}N. Zhu, W.L. Goh and K.S. Yeo, ‚ÄúAn enhanced low-power high-speed adder for error-tolerant application,‚Äù in Proc. ISIC‚Äô09, pp. 69‚Äì72, 2009.
\bibitem{}N. Zhu, W.L. Goh and K.S. Yeo, ‚ÄúUltra low-power high-speed flexible probabilistic adder for error-tolerant applications,‚Äù in Proc. Intl. SoC Design Conf., pp. 393‚Äì396, 2011.
\bibitem{}H.R. Mahdiani, A. Ahmadi, S.M. Fakhraie, C. Lucas, ‚ÄúBio-inspired imprecise computational blocks for efficient vlsi implementation of soft-computing applications,‚Äù IEEE Trans. Circuits and Systems I: Regular
Papers, vol. 57, no. 4, pp. 850-862, April 2010.
\bibitem{}J. Miao, K. He, A. Gerstlauer and M. Orshansky ‚ÄúModeling and synthesis of quality-energy optimal approximate adders,‚Äù in Proc.
ICCAD, pp. 728, 2012.
\bibitem{}C. Lui, A Low-Power, "High-Performance Approximate Multiplier with Configurable Partial Error Recovery," EDAA, 2014.
\bibitem{}Kyaw, K, "Low-Power High-Speed Multiplier For Error≠ Tolerant Application," EDSSC, 2010.

\bibitem{}Farshchi, F, "New Approximate Multplier for Low Power Digital Signal Processing, " IEEE, 2013. 
\bibitem{} Sugawara, Y, "System for Automatic Generation of Parallel Multipliers over Galois Fields," IEEE, 2015.
\bibitem{} LeCun, Y, "Gradient-Based Learning Applied to Document Recognition," IEEE, 1998.

\end{thebibliography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Emily_Pic}}]{Emily Naviasky}
Emily Naviasky earned a Bachelors in Electrical Engineering and in Computer Science at the University of Maryland. Presently, she is a Graduate Student at UC Berkeley interested in analog integrated circuits. She is upside-down, and has an innate resilience to imperfections in precision.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Keertana_Pic}}]{Keertana Settaluri}
is currently a first year doctoral student at UC Berkeley. She has gained immense popularity in the recent decade, and is small, practical, and well established. 
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


