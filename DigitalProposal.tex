\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\ifCLASSINFOpdf
\else
 \fi
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{High Speed Energy-Efficient Convolutional Neural Network with Approximate Computing}

\author{\IEEEauthorblockN{Keertana Settaluri}
\IEEEauthorblockA{School of Electrical and Computer Engineering\\
University of California, Berkeley\\
Email: ksettaluri6@berkeley.edu}
\and
\IEEEauthorblockN{Emily Naviasky}
\IEEEauthorblockA{School of Electrical and Computer Engineering\\
University of California, Berkeley\\
Email: enaviasky@berkeley.edu}
}

\maketitle

\begin{abstract}
%\boldmath
\blindtext[1]
\end{abstract}

\begin{IEEEkeywords}
Convolutional Neural Network, Approximate Computing, Energy Efficiency
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
	\indent Deep neural networks have gained immense popularity in the recent decade, primarily in complex learning applications such as computer vision, speech recognition, and natural language processing. However, as problem complexity increases, the size of neural networks and the sheer amount of computation required drastically increases. Understandably, a significant amount of time and research is being used to develop a more efficient, less power intensive, and faster hardware implementation of neural networks. One way to attempt to decrease the massive increase of neurons is to change the structure of the network. For example, convolutional neural networks, which have gained popularity recently in image and signal processing applications, were developed to deal with data, such as a picture, that would otherwise require an infeasible number of neurons in a traditional neural network. However, convolutional neural networks are still extremely computationally intensive.[class] \\
	\indent Convolutional neural networks (convNets), in particular, are widely used in many computer vision applications such as pattern recognition and image classification. Current smart phone applications, such as Google translate [9], make use of these networks through the use of a mobile processor's SIMD instructions. In power conscious applications such as these, using specialized hardware would enable processors not necessarily built for convNets to compute faster and more energy efficient computations. Low power neural network hardware accelerators have been developed, but their speed usually comes at the expense of generalization, as in the case of Eyeriss[eyriss]. This hardware specific optimization limits the application to only the one in which it was designed. \\	
	\indent ConvNets have an innate resilience to imperfections in precision due to the nature of the noisy data which they are used to interpret. Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of multiply and accumulate operations in order to speed up a system and save power. Thus, several works have investigated the use of approximate computing to trade unnecessary accuracy for power and speed in neural networks. This work seeks to investigate different implementations of approximate computing applied specifically to convNets. \\
	

	
\section{Problem Description}
	\indent Using a convNet to classify a single, small image can be very computationally expensive. A typical convNet, such as AlexNet [kriz], uses 2.3 million weights, and requires 666 million MACs per 227x227 image. A even more intensive implementation, VGG16 [simoy], uses 14.7 million weights, and requires a staggering 15.3 billion MACs per 224x224 image.\\
%-is it worth doing approximate computing in CNN?
	\indent It is an easy decision to want to trade some of the unnecessary accuracy in such a computationally expensive algorithm for power savings and faster computation. However, accuracy for speed and power is not a decision made in a vacuum. Area on silicon is expensive and an entire block for approximate hardware would have to have significant enough power savings to be worth the area. It is the goal of this paper to explore whether CNN are an appropriate application for approximate computing in a quantitative way. We will begin <fuck not using I or we, fix it later> by examining CNNs and approximate computing in more detail so that we can <talk intelligently about them later.>\\

\subsection{Convolutional Neural Nets}
%-What is a CNN? 
TODO NNS
	\indent Neural networks are BLAH training backprop meshes of nodes\\
	
TODO CNNs
	\indent ConvNets like general neural networks are used to deal with data that is infeasible for normal neural net arrangements. Neurons in a convNets are arranged in three dimensions, which reduce the number of neurons and weights, making them better able to deal with matrix shaped data such as images. BLAH three stages, pooling and stuff. draw parallel between whatever stage is like a normal net.  \\
	
	\indent ConvNets already implement approximate computing at a software level[9]. On a hardware level, most research in to power savings and approximate computation of neural networks focus on general neural nets as opposed to convNets, but have found a marked improvement in power and speed without sacrificing significant accuracy of the net. Implementations such as AxNN [axnn] show varying degrees of power savings for minimal loss in accuracy. AxNN uses back propagation--- an algorithm used to change weights when training neural nets--- to identify less important nodes and decrease their accuracy by shortening the bit-width used in computation. This paper claims to achieve up to 1.9X energy benefits for less than 1\% in output quality, and an even greater factor of 2.3X when 7.5\% loss is permissible at the output[]. This conclusion validates that approximate computing can result in significant power savings. However, there are many approximate computing algorithms, besides reducing bit-width, that show even more promise for power to accuracy trade-off.\\
	

\subsection{Approximate Computing}

	\indent Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of an operation in order to speed up a system and save power. Approximate computing can occur at a single operation level or at a algorithmic level. Algorithm level approximate computing has been used in applications that respond well to early termination when an answer is close enough, such as <sorting?>[] and Support Vector Machines[]. This paper implement approximate computing at an operational level.\\
	TODO Error calc
	\indent 	As seen in most approximate adder and multipliers, the extent to which power is saved is proportionally related to the error injected into the system. The most common metrics for quantifying these errors are error rate (ER) and error significance (ES), which represent the fraction of incorrect outputs versus total number of inputs and the numerical difference from an incorrect and correct output, respectively [7]. \\
	TODO ADDERS LIT REV
	\indent Many implementations of approximate adders and multipliers exist. Adders such as Approximate Mirror Adders[6387646] and Error Tolerant Adders[zhu] predict  vary from using voltage scaling without scaling clock frequency to algorithmically ignoring lower and less important bits[][][]. \\
	
TODO MULTIPLIERS LIT REV
	\indent Considerably less effort has been put into designing approximate multipliers. Previous work explores removing the partial products of the least significant half of the bits in a multiplier and replacing them with 1's or 0's depending on the two multiplicands. Simulations in 0.18-$\mu$ technology estimate a minimum of 50\% savings in power dissipation while still having considerable accuracy [].\\
	\indent Approximate multipliers might implement normal multiplier topologies with approximate adders[], but those are generally worse than multipliers that implement approximation at the multiplication level<not a huge fan of that wording>[]. Architectural \\



\section{Implementation}
	\indent Combining convNets and approximate computing has already been shown to save power and improve speed. AxNN[axnn] examined approximate computing in neural net application and saw about x2 power improvement for less than 1\% quality loss. In particular AxNN noted that neural nets are a uniquely appropriate application for approximate computing. They used the fact that certain neurons in a net are less sensitive than others when determining the correct answer than others. In addition, the way that neural nets are trained naturally corrects for and decreases noise and error. AxNN also found that selection of only less sensitive nodes results in lower power per accuracy loss. This improvement is constant over error rate. TODO REWORD
They also tested decreasing accuracy uniformly over nodes and found that power per error rate is decreased. However, the accuracy vs. power savings of both insensitive and uniform node selection decrease at a similar rate. Therefore, 
Therefore, power savings vs. error rate can be evaluated by applying approximation uniformly over neurons and the AxNN algorithmic selection of nodes would result in an improved result. [axnn] \\ 


	\indent Axnn saw improvements using bit-width variation to impose approximation. Bit-width approximation is a very power efficient approximation for computing, however, it introduces a very large mean error. Lower-Part-OR Adder is more power expensive but introduces only the addition of one or gate per lower bit. It's mean error, however, is a very small constant. Replacing bit-width approximation with Lower-Part-OR Adders should result in significantly higher accuracy which should allow the net to be pushed to lower power consumption for the same error rate. The same argument can be made of the XXX multiplier. BLALALALALALAL TODO\\

	\indent We will implement Lower-Part-OR approximate adders as well as XXX multiplier in 32nm LP CMOS technology, and use Synthesis simulations to calculate average power per operation and confirm mean error. We will also implement an accurate adder and multiplier as well as a  bit-width varying approximate adder to compare performance metrics.\\

	\indent ConvNets are error-tolerant; it is difficult to judge the accuracy of them without testing data in the net and observing the change in  the accuracy with which the data is classified. We are going to use the small, practical, and well-established convolutional neural net, LeNet, which is a small architecture from the 1980s that is used to recognize handwritten digits. TODO MORE LENET GRADIENT DESCENT OR SOMETHING In addition there is a deep learning python framework from UC Berkeley already in place called Caffe. We will adapt LeNet in Caffe, artificially introducing error into neuron operations to match the approximation method from our simulated hardware. Using Caffe we will explore introducing error into different layers of the convolutional net, to see if one, two, or all of the layers are equally error tolerant. By varying the accuracy of the adders and multipliers in each neuron and testing data in the net we will develop a metric and determine the accuracy of the net as a whole.\\

	\indent Again using python, the number of adds and multiplications performed will be compiled and combined with the power numbers from the hardware simulation to obtain a approximate power measurement. \\

	\indent We expect our convNet to be able to tolerate some percentage of error in each node's arithmetic, and we are interested to see the relation between error of a neuron versus accuracy of the convNet as a whole. Despite the fact that most works do not focus on this relationship between neuron error and net accuracy, we feel that this is an interesting correlation to investigate. Which would enable generalizations and rules-of-thumb to be made about arithmetic error tolerance in many neural networks.\\
	

\section{Conclusion}
%-WOOOO HOPE IT WORKS
% At what point is this useful?
We expect to see power and speed improvement by performing more <error-tolerant?> approximate computation into CNN application.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}

\end{document}



