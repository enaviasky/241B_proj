
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}

\ifCLASSINFOpdf

\else

\fi
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{High Speed Energy-Efficient Convolutional Neural Network with Approximate Computing}

\author{Keertana Settaluri,
        Emily Naviasky\\% <-this % stops a space
        ksettaluri6@berkeley.edu, enaviasky@berkeley.edu,\\
        Department of Electrical and Computer Engineering, UC Berkeley}

\markboth{Journal of EE241B ,~Vol.~$\pi$, No.~0, March~2016}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle


\begin{abstract}
Convolutional neural networks are used in many vision applications such as pattern recognition and image classification. The amount of computation they require however, is immense. Approximate computing is a technique that reduces accuracy in favor of power savings and increased performance. Because convNets are innately resilient to error, using approximate computing blocks in convNet hardware proves to be very beneficial. This paper assesses several approximate adder and multiplier configurations, and chooses the Lower-part-OR adder and Broken-Array Booth multiplier to implement for each neural operation. Using UC Berkeley's Caffe and Cadence tools, we propose a new hardware approximate convNet accelerator that predicts promising power and performance metrics, with minimal tradeoff in accuracy. 
\end{abstract}

\begin{IEEEkeywords}
Convolutional Neural Network, Approximate Computing, Energy Efficiency
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
	\indent Deep neural networks have gained immense popularity in the recent decade, primarily in complex learning applications such as computer vision, speech recognition, and natural language processing. However, as problems become more complex, the size and sheer amount of computation required for neural networks drastically increases. Understandably, a significant amount of time and research is being used to develop a more efficient, less power intensive, and faster implementation of neural networks. \\
	
	\indent In particular, architectural modifications have been, and are currently being explored as a possible attempt to dynamically reduce the amount of neurons and connections in a network. Convolutional neural networks (convNets), which are widely used in many computer vision applications such as pattern recognition and image classification, are an excellent example. They were specifically developed to deal with a large input dataset, which would require an infeasible number of neurons in a traditional neural net. Despite making immense strides in computational efficiency, convNet operations still range in the millions or greater.[1] \\
	
	\indent The push to develop better convNets is largely fueled by their potential use in the consumer market. Current smart phone applications, such as Google translate [2], utilize these networks through the use of a mobile processor's SIMD instructions. Including more efficient hardware in their design would enable power conscious devices such as these to utilize convNets yet still be energy efficient. Though low power neural network hardware accelerators have been developed, their speed usually comes at the expense of generalization, as in the case of Eyeriss[3]. \\
		
	\indent ConvNets have an innate resilience to imperfections in precision due to the nature of the noisy data which they are used to interpret, which makes them an ideal application for approximate computation[12]. This paper seeks to explore hardware specific approximate computing as an optimization approach for convNet applications that efficiently reduces the precision of multiply and accumulate operations in order to speed up a system and save power. Several works have investigated the use of approximate computing to trade unnecessary accuracy for power and speed in neural networks. This work seeks to investigate different implementations of approximate computing applied specifically to convNets. 

\section{Problem Description}
	\indent Using a convNet to classify a single, small image can be very computationally expensive. A typical convNet, such as AlexNet [1], uses 2.3 million weights, and requires 666 million MACs per 227x227 image. An even more intensive implementation, VGG16 [4], uses 14.7 million weights, and requires a staggering 15.3 billion MACs per 224x224 image.\\
	
	\indent Choosing to tradeoff unnecessary accuracy for power savings and faster operation is not a difficult design decision. However, this decision must also take into consideration area limitations. Because silicon is expensive, designing an entire block for approximate hardware would have to imply significant power savings to be worth the area. Thus it is beneficial to explore whether convNets are an appropriate application for approximate computing. This paper will begin by examining convNets and approximate computing in more detail.
	
\subsection{Convolutional Neural Nets}
	\indent In general, neural networks are a classification algorithm well suited for noisy data[12]. They are composed of a highly connected mesh of nodes, where each connection represents some weight. Each node adds the weighted combination of the nodes from the previous layer. If the sum of weights is above a certain threshold function, then the node is activated and contributes its weight to the next layer; whereas, an inactivate node has a weight of zero. In this way, neural nets can represent very complex functions. The more layers, nodes, and interconnections there are, the more complex the representable functions are. However, larger and more complex nets require significantly more training. The training of these neural nets is typically done using back propagation and depending on the correctness of an output, training revisits all of the activated interconnects and negatively/positively reinforces the incorrect/correct answer by changing the mesh's weights. \\
	
	\indent ConvNets use learnable weights and biases similar to regular neural networks, however the arrangement of neurons in three dimensions makes them uniquely suited for processing input images. Because each neuron in a convNet only connects to a local region of the input volume, as opposed a traditional neural network, where every neuron is connected to each other, convNets are ideal for dealing with matrix shaped data such as images.  A typical convNet architecture consists of a convolutional layer, pooling layer, and fully-connected layer. These layers compute the dot product between the weights and the local region where a neuron is located, downsampling of operations in the spatial dimension, and computing class scores, respectively. Furthermore, convNets also utilize an activation function between the convolutional and pooling layers in order to increase nonlinearity which thereby makes training faster. \\
	
	\indent ConvNets already implement approximate computing at a software level[2]. On a hardware level, most research into power savings and approximate computation of neural networks focuses on general neural nets as opposed to convNets, but have found a marked improvement in power and speed without sacrificing significant accuracy of the net. Implementations such as AxNN [6] show varying degrees of power savings for minimal loss in accuracy. AxNN uses back propagation--- an algorithm used to change weights when training neural nets--- to identify less important nodes and decrease their accuracy by shortening the bit-width used in computation. This paper claims to achieve up to 1.9X energy benefits for less than 1\% loss of net accuracy, and an even greater factor of 2.3X when 7.5\% loss is permissible at the output[6]. This result validates that approximate computing can result in significant power savings. However, there are many approximate computing algorithms besides reducing bit-width, that show even more promise for power to accuracy trade-off.

\subsection{Approximate Computing}

	\indent Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of an operation in order to speed up a system and save power. Approximate computing can occur at an operational or algorithmic level. Algorithmic level approximate computing has been thoroughly explored and used in applications that respond well to early termination when an answer is close enough, such as digital filters[7] and Support Vector Machines[8]. However, this paper will focus on approximate computing at an operational level.\\
	

	\subsubsection{Error Quantification}
	\indent 	There are several metrics for quantifying the error from approximate computing. Error rate (ER) is the probability of an error occurring and is common among many approximate algorithms[6]. Error significance (ES) determines how far the approximate answer is from its accurate counterpart. Error mean (EM), error mean square, hamming distance, error min/max, or some combination thereof, are typical methods of characterizing ES; choosing which metric to calculate largely depends on the application[8]. The following analysis of approximate operations will use error rate and error mean. For a neural net application, the thresholding function inside of each neuron eliminates some small constant error, but is unable to handle infrequent but large errors. In addition, because each neuron sums a very large number of weighted inputs, a small and frequent error will place the average error close to the mean. Therefore, a higher ER is acceptable for a neural net application, but ES must be smaller. \\	
	
	\subsubsection{Approximate Adders}
	\indent Many implementations of approximate adders exist. Voltage scaling, for example, is a technique that reduces supply voltage without scaling frequency, and introduces error to both adders and multipliers because timing constraints are not met. Voltage scaled approximation is an effective method of decreasing power but needs to be carefully designed to ensure that significant bits behave well under scaled voltage conditions without risking a large ES.[8] In a similar vein, Speculative Latency Adders[9] allow delay to dictate error; these adders rely on the fact that the probability of a worst-case carry chain is small and will terminate early. This results in increased performance but minimal power savings, especially because this method is usually paired with error recovery.[8] The rest of this section examines adders with logical as opposed to transistor level error, such as the Error Tolerant Adder[10][11] and Lower-part-OR adder[12], as they seem to indicate better efficiency metrics. \\
	
	\indent The Error Tolerant Adder I (ETA 1) divides the adder into subsections and cuts the carry propagation chain between each section[10]. Some of the error induced by ignoring carries is corrected in subsequent versions of the Error Tolerant Adders (ETA II/III) by taking just the upper bits of the previous subsection into account[11]. Akin to the Error Tolerant Adders, the Dithering Adder[13] cuts the carry chain, but alternates the value of the replacement carry-in after each addition. This causes the addition to be either an an over or under approximation and makes the Dithering Adder well suited for accumulation. The ER of Error Tolerant Adders is between 20-25\% (depending on various options) but no mention is made of EM. Although the ETA estimates a 50\% reduction in power consumption compared to a typical adder, presumably because the answer is calculated in half the time, other adders indicate better power efficiency.\\
	
	\indent The Lower-part-OR (LPO) takes removal of lower bit carries to an extreme[12]. The LPO adder defines some number of lower bits and uses OR instead of XOR. No carries are propagated in these lower bits; only a single logical AND is performed on the most significant bits of the lower-part to choose a carry-in for the upper part. The paper[12] does not explore power savings, but the lack of carry propagation in the lower bits suggests somewhat significant power savings compared to other approximate adders that propagate carries through the lower bits. The ER for this adder is extremely high, and the minimum and maximum errors increase with a longer lower-part length. It is interesting to note, however, that the EM is completely independent of the word length or the lower-part length. The probability and magnitude of error in all bits except the lowest is equal, and thus the EM only -0.25. The lower power and low EM of this adder make it particularly appropriate for neural nets, and the paper[12]  even tests this adder on a neural net for face recognition. They found that the neural net could classify well with all of its adders approximating the lowest 9 of a 13-bit number. These characteristics make LPO a very good choice for a convNet application.\\
	  
\subsubsection{Approximate Multipliers}
	\indent Most research in approximate multipliers extends from existing approximate adder implementations. Solely replacing every partial product with any version of an approximate adder, however, creates too much error. Error correction methods or different, multiplier specific implementations have been proven to be more effective. For example, a multiplier with configurable partial recovery error detection was created by using an input pre-processing approximate adder for all partial products. This adder exchanges input bits in the case where one input bit is a 0 and the other is a 1. This stacking provides a means to cut the carry propagation chain, predicting up to a 69\% savings in power and up to 20\% decrease in delay. Though this multiplier predicts near negligible ES compared to other approximate multipliers because of error detection logic, this additional sub-block creates a larger area overhead. [14]\\
	
	\indent Another implementation manipulates the partial product by splitting the input bits evenly into two different components. The most significant bits are left untouched, and regular partial products are calculated. Carry propagate is removed for the lower order bits, and every bit position from left to right (MSB to LSB) is checked to see if the product of any two bits is 1. If this partial product is 1, every bit after and to the right of this location is set to 1. Simulations in 0.18-$\mu$ technology estimate a 50-97\% savings in power [15], but only several cases were considered in determining this power metric. In addition, the ES is reported to be 94-100\% [15]. Again however, these values only included five different input patterns. It is safe to say, therefore, that actual implementation is unlikely to meet the reported power savings to error ratio. \\
	
	\indent A promising approach uses a Broken-Array Booth Multiplier and claims up to a 58\% savings in power. The multiplier works by defining a Vertical Breaking Level (VBL), where any partial products located to the right of this line are not calculated. Using a world length of 12, the mean error proposed by this paper ranges from -3.50 for a VBL of three to -789 for a VBL of nine. The VBL for the 58\% power reduction was set to 15 in a 16x16 multiplication, and the corresponding tradeoff in accuracy is minimal[16]. 
	
\section{Implementation}
	\indent Combining convNets and approximate computing has already shown power and performance improvements. In particular, AxNN[6] examined approximate computing in a neural net application, because they believed neural nets are uniquely suited for approximate computing, and saw almost 2X power improvement for less than 1\% loss in quality. Specifically, they made use of the fact that certain neurons in a net are less sensitive than others when determining the correct answer. They replaced these less sensitive nodes with approximate nodes. In addition, they took advantage of the fact that training a neural net naturally corrects for and decreases noise and error, by adding a short re-training period after introducing the approximate nodes. This allowed the net to correct for approximations that were introduced. Comparatively, AxNN found that replacing every node with approximate hardware resulted in a vertically shifted power versus accuracy loss curve when compared to only the less sensitive nodes being replaced[6]. This implies that in our work, we can apply approximate computing to all of the nodes regardless of their sensitivity to obtain a good benchmark. Afterwards, application of more careful neuron selection will result in further improvement.\\ 

	\indent AxNN imposed approximation by varying bit-width. Though bit-width approximation is a very power efficient technique for computing, it introduces a large mean error. The Lower-Part-OR Adder is somewhat more power expensive but introduces only the addition of one OR gate per lower bit. Its mean error, however, is significantly smaller. Replacing bit-width approximation with LPO adders should result in significantly higher accuracy, and thereby allow the net to be pushed to lower power consumption for the same error rate. Using similar inferences, we expect that the Broken-Array Booth Multiplier, which has the most promising power per accuracy metric compared to other multipliers, will function at lower power for equivalent error in the neural network as well.\\

	\indent We will implement Lower-Part-OR approximate adder as well as Broken-Array Booth multiplier in 32nm LP CMOS technology, and use Cadence simulations to calculate average power per operation and confirm mean error. We will also implement an accurate adder and multiplier as well as a  bit-width varying approximate adder to compare performance metrics.\\

	\indent Because convNets are error tolerant, it is difficult to judge their accuracy without physically testing data in the net, and correspondingly observing the change in accuracy with which the data is classified. In our implementation, we choose to utilize the small, practical, and well-established convolutional neural net architecture, LeNet[17], which was created in the 1980s to recognize handwritten digits. LeNet makes use of the, now typical, Stochastic Gradient Descent (SGD) algorithm and Graph Transformer Networks (GTN), and defined the modern framework for modern convNet architectures. Any power and performance metrics we obtain using LeNet, therefore, are still relevant and practical to present convNets. In addition, UC Berkeley has created a deep learning framework in Python called Caffe that implements a slightly modified version of LeNet. By adapting this architecture, we will artificially introduce error in neuron operations to match the approximation method from our simulated hardware. In addition, we will explore introducing error in different layers of the convolutional net, specifically to see if one, two, or all of the layers are equally error tolerant. By varying the accuracy of the adders and multipliers in each neuron and testing data in the net, we will develop a metric and determine the accuracy of the convNet as a whole. Using Python, the number of adds and multiplications performed will be compiled and combined with the power numbers from the hardware simulation to obtain an approximate power measurement. \\

	\indent We expect our convNet to be able to tolerate some percentage of error in each node's arithmetic, and are interested in seeing the relation between error of a neuron versus accuracy of the convNet as a whole. Despite the fact that most works do not focus on this relationship between neuron error and net accuracy, we feel that this is an interesting correlation to investigate, and would enable generalizations and rules-of-thumb to be made about arithmetic error tolerance in many neural networks.\\

\section{Conclusion}
	\indent ConvNets are a prevalent, power intensive applications, and based on examination of previous works, they are also an excellent application for approximate computation. Approximate computing has already shown improvements in power and speed, and implementing an LPO approximate adder and Broken-Array Booth Multiplier inside of neurons should result in an improvement over these factors. We will compare power using a Synopsys generated model of approximate adders and multipliers, and judge error resiliency of the approximate net using Caffe. We expect an improvement in average power versus accuracy of the network. \\

\vspace{63mm} 
\begin{thebibliography}{1}

\bibitem{}
 A. Krizhevsky, I. Sutskever, G. E. Hinton, "ImageNet Classification with Deep Convolutional Neural Networks," Neural Information Processing Systems, pp. 1097â€“1105, 2012.	
\bibitem{} Otavio Good, â€œHow Google Translate squeezes deep learning onto a phone,â€ Google Translate, July 29, 2015. http://googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html
\bibitem{} Y. Chen, T. Krishna , J. Emer, â€œEyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks,â€ ISSCC 2016.
\bibitem{} K. Simonyan, A. Zisserman, â€œVery Deep Convolutional Networks for Large- Scale Image Recognition,â€ CoRR, 2014. 
\bibitem{} Karpathy. A, "CS231n Convolutional Neural Networks for Visual Recognition," January, 2016. http://cs231n.github.io
\bibitem{} S. Venkataramani, â€œAxNN: Energy Efficient Neuromorphic System using Approximate Computing,â€ ISLPED 2014.
\bibitem{} K. Roy, A. Raghunathan, â€œApproximate Computing: An Energy-Efficient Computing Technique for Error Resilient Applications,â€ Computer Society Annual Symp. on VLSI, 2015.
\bibitem{} J. Han, M. Orshansky, â€œApproximate Computing: An Emerging Paradigm For Energy-Efficient Design,â€ ETS, 2013.
\bibitem{} S.-L. Lu, â€œSpeeding up processing with approximation circuits,â€
Computer, vol. 37, no. 3, pp. 67-73, 2004.
\bibitem{}N. Zhu, W.L. Goh and K.S. Yeo, â€œAn enhanced low-power high-speed adder for error-tolerant application,â€ in Proc. ISICâ€™09, pp. 69â€“72, 2009.
\bibitem{}N. Zhu, W.L. Goh and K.S. Yeo, â€œUltra low-power high-speed flexible probabilistic adder for error-tolerant applications,â€ in Proc. Intl. SoC Design Conf., pp. 393â€“396, 2011.
\bibitem{}H.R. Mahdiani, A. Ahmadi, S.M. Fakhraie, C. Lucas, â€œBio-inspired imprecise computational blocks for efficient vlsi implementation of soft-computing applications,â€ IEEE Trans. Circuits and Systems I: Regular
Papers, vol. 57, no. 4, pp. 850-862, April 2010.
\bibitem{}J. Miao, K. He, A. Gerstlauer and M. Orshansky â€œModeling and synthesis of quality-energy optimal approximate adders,â€ in Proc.
ICCAD, pp. 728, 2012.
\bibitem{}C. Lui, A Low-Power, "High-Performance Approximate Multiplier with Configurable Partial Error Recovery," EDAA, 2014.
\bibitem{}Kyaw, K, "Low-Power High-Speed Multiplier For Error­ Tolerant Application," EDSSC, 2010.

\bibitem{}Farshchi, F, "New Approximate Multplier for Low Power Digital Signal Processing, " IEEE, 2013. 
\bibitem{} LeCun, Y, "Gradient-Based Learning Applied to Document Recognition," IEEE, 1998.

\end{thebibliography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Emily_Pic}}]{Emily Naviasky}
Emily Naviasky earned a Bachelors in Electrical Engineering and in Computer Science at the University of Maryland. Presently, she is a Graduate Student at UC Berkeley interested in analog integrated circuits. She is upside-down, and has an innate resilience to imperfections in precision.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Keertana_Pic}}]{Keertana Settaluri}
is currently a first year doctoral student at UC Berkeley. She has gained immense popularity in the recent decade, and is small, practical, and well established. 
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


