\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\ifCLASSINFOpdf
\else
 \fi
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{High Speed Energy-Efficient Convolutional Neural Network with Approximate Computing}

\author{\IEEEauthorblockN{Keertana Settaluri}
\IEEEauthorblockA{School of Electrical and Computer Engineering\\
University of California, Berkeley\\
Email: ksettaluri6@berkeley.edu}
\and
\IEEEauthorblockN{Emily Naviasky}
\IEEEauthorblockA{School of Electrical and Computer Engineering\\
University of California, Berkeley\\
Email: enaviasky@berkeley.edu}
}

\maketitle

\begin{abstract}
%\boldmath
\blindtext[1]
\end{abstract}

\begin{IEEEkeywords}
Convolutional Neural Network, Approximate Computing, Energy Efficiency
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
	Neural networks have gained immense popularity in recent decades, primarily because of their utility in applications such as computer vision, speech recognition, and natural language processing. Creating a hardware implementation of a neural network, however, is extremely difficult due to the sheer amount of computation required. For example, AlexNet [1] uses 2.3 million weights, and requires 666 million MACs per 227x227 image. VGG16 [2] uses 14.7 million weights, and requires a staggering 15.3 billion MACs per 224x224 image. Understandably, a significant amount of time and research is being used to develop a more efficient, less power intensive, and faster hardware implementation of a neural network.\\
	\indent Although low power implementations have been developed, they are usually at the expense of generalization, wherein specialized dataflow and hardware are developed to minimize data movement and skip unnecessary read or write operations, as in the case of Eyeriss [3]. This hardware specific optimization limits the application to only the one in which it was designed.

\subsection{Approximate Computing}
	Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of multiply and accumulate operations in order to speed up a system and save power. \\
	\indent Many approximate adders and multipliers have been designed to reduce energy at the expense of precision. The Approximate Mirror Adder (AMA), for example, removes transistors at the logic level, thus reducing node capacitance while inducing error in the output, and predicts up to 69\% [4] savings in power when four out of 16 bits are implemented using an AMA. Voltage Overscaling (VOS) is another technique that reduces the supply voltage while trading off for lower SNR. Several papers have explored VOS designed circuits, and one decoder implementation claims a 22.5\% savings in energy while minimally affecting SNR [5]. \\
	\indent Considerably less effort has been put into designing approximate multipliers. Previous work explores removing the partial products of the least significant half of the bits in a multiplier and replacing them with 1's or 0's depending on the two multiplicands. Simulations in 0.18-$\mu$ technology estimate a minimum of 50\% savings in power dissipation while still having considerable accuracy [6].

\subsection{Approximate Computing in Neural Networks}
	Neural networks provide the perfect platform for approximate computing, as they have an innate resilience to imperfections in precision. Implementations such as AxNN [7] and Liu's Deep Neural Network [8] show varying degrees of power savings for minimal loss in accuracy. AxNN claims up to 1.9X energy benefits for no loss in output quality, and an even greater factor of 2.3X when 7.5\% loss is permissible at the output through the use of approximate neurons and incremental retraining [7]. Both of these utilizations however, utilize a single approximation algorithm. \\
	
	Shit you need to mention:\\
		1) Phones use neural networks \\
		2) Phones use approx in conv neural nets to carry out their baby implementations\\
		3) Neural net applications important \\
		4) convNets are used in computer vision\\
		
	\indent The need for faster and more efficient methods of utilizing neural networks is immense. In particular, convolutional neural networks (convNets) are widely used in many computer vision applications such as pattern recognition and image classification. Current smart phone technology makes use of these networks in order to implement deep learning. Though smart phone In applications where hardware capabilities are limited, such as in cellular devices, the efficient application of a neural network could open the door to more useful processing.  \\
	Present how its necessary for hardware. This is being done in phones, but if theres a way to do processing faster more capabilities on the phone can be a thing. 
	This paper presents this blah blah 
	Make sure to mention possible generalization of the system. 
	
	
	
\section{Problem Description}
%-is it worth doing approximate computing in CNN?
CNN are a error tolerant application that are prevalent in many power conscious applications even though they require a large amount of computation to train and use. It is an easy decision to want to trade some of that unnecessary accuracy for power savings and faster computation. However, accuracy for speed and power is not a decision made in a vacuum. Area on silicon is expensive and an entire block for approximate hardware would have to have significant enough power savings to be worth the area. <Was there more reasons besides area? I can't remember> It is the goal of this paper to explore whether CNN are an appropriate application for approximate computing in a quantitative way. We will begin <fuck not using I or we, fix it later> by examining CNNs and approximate computing in more detail so that we can <talk intelligently about them later.>
\subsection{Convolutional Neural Nets}
%-What is a CNN? 
CNNs address the problem common to using neural nets in computer vision, which is that it requires an infeasible number of neural nodes<?> in order to process a very small image if each pixel is given it's own node. <lots of nn stuff to ask keertana>
%	-CNNs are for vision
%	-how they differ from normal NNs (course thing/papers on CNNs)
%-algorithms and noise that are typically in the system
%-FOM, how their goodness is judged(error in NNs)
\subsection{Approximate Computing}
%-What is approximate computing?
There are many implementations of approximate adders and multipliers, or approximate computing can be implemented on a architectural level for application specific computing blocks. Approximate adders vary from using voltage scaling without scaling clock frequency to algorithmically ignoring lower and less important bits[][][]. Approximate multipliers might implement normal multiplier topologies with approximate adders[], but those are generally worse than multipliers that implement approximation at the multiplication level<not a huge fan of that wording>[]. Architectural level approximate computing has been used in applications that respond well to early termination when an answer is close enough, such as <sorting?>[] and Support Vector Machines[].

%	-Describe approx comp we want to use
%		-this one is superior cuz BLAH
<We> will examine approximate adders and multipliers rather than architecture level approximation. Some of the best performing adders and multipliers in recent publications are <blahhhhhhh, lower-part-or-adders[bio-inspired], ETA[06138614,05403865]?>. 

%	-what are some typical numbers for improvement
%		-they saw X power/ speed improvement vs error
%	-how we are going to model and judge FOM
%-paper on approximate CNN
<Does this belong here?>Other works that deal specifically with neural nets[axnn] have seen around 2x improvement in power with little degradation in accuracy, just by changing bit-width in the computation<meh wording>. 
%-They look like a good fit + decent ending sentence

\section{Solution}
%-Why should approximate computing in CNNs work?
%	-quote some power numbers or something


%-judge workspace
%		-other works for X loss in accuracy see gain in power/speed
%		-graph power vs accuracy/ accuracy necessary
%			-how much accuracy does a NN need anyway?
%-paper on approximate CNN
%	-more depth, depending on how useful it was
Axnn examined approximate computing in neural net application. They saw about 2x power improvement for less than percent quality loss. In particular axnn noted that neural nets are a uniquely appropriate application for approximate computing. They used the fact that certain neurons in a net are less sensitive than others when determining the correct answer than others. In addition, the way that neural nets are trained naturally corrects for and decreases noise and error. Axnn also found that conscious selection of less sensitive nodes does result in lower power per accuracy loss, but that the benefit reflects a similar decrease in power per accuracy as uniform approximation. Therefore, we could apply their selection later and get even better improvements later.<keertana, halp, what are words? do words make sense> [axnn] 

Axnn saw improvements using bit-width variation to impose approximation<uhgwords>. <XXX> adder saw <YYY> improvement over typical methods such as <omg I know nothing>.<We> expect to see even further improvements testing other methods of more <error tolerant?> approximation computing.



%-What we hope to find
%	-Power and speed improvement is significant versus loss of accuracy
%	-Reiterate main question: is it worth doing approximate computing in CNN?

\section{Description of Experimental Work}
From the above exploration of CNNs and AC, <we> are convinced that applying AC to CNN is will result in significant power and speed savings with tolerable accuracy loss. For this project, <we> will explore the specific power savings and speed increase from implementing <XXX> approximate adder <more detail> and multipliers <more detail> as well as approximate adder and multiplier implemented using voltage scaling [][][]. 
%-Experimental setup
%	-verilog for power numbers, size, error rate
<We> will implement approximate adders and multipliers in 65nm<is that what we are using in this class?> technology, and use simulations to calculate the average power per operation.
%-python, lanet (reference more course thing), blah blah don’t wanna train one
%	-plug in error rate/introduce same amount of error
%-see if shit still works
%	- judge goodness
CNNs are error-tolerant; it is difficult to judge the accuracy of them without observing the change in classification accuracy over a set of test data. It can also be extremely time intensive to create and train a CNN. Thus, we are going to use the framework already in place called <Cafe?> and a pre-trained CNN. We will artificially introduce error to the software calculation that matches that from our simulated hardware, then see the number of adds and multiplications performed and the accuracy of the net as a whole.
%-How are we going to judge worth or not worth
%-FOM
%	-what are we doing in particular to judge power/speed/usability of NN
We expect our CNN to be able to tolerate some percentage of error in each node's arithmetic, and we are interested to see what percentage error of CNN classification this results in. <The following may or may not be something we actually want to say> We cannot find any other work that examines how error per node propagates to the accuracy of the CNN. This is probably due to the accuracy of CNNs being largely dependent on a large number of factors including data set, topology of the net, order and algorithm of training, and etc. Accuracy of the nodes does not necessarily map from one CNN to another, but it could be useful to see how much error it introduces to the same net with and without AC.

<<<<<<< HEAD
\section{Conclusion}
\blindtext
=======

>>>>>>> 621641f5aff10c1c5beaa0072857a990f5f83647


%*** IDK ABOUT THIS PARAGRAPH *** As seen in most approximate adder and multipliers, the extent to which power is saved is proportionally related to the error injected into the system. The most common metrics for quantifying these errors are error rate (ER) and error significance (ES), which represent the fraction of incorrect outputs versus total number of inputs and the numerical difference from an incorrect and correct output, respectively [7]. 

\section{Conclusion}
%-WOOOO HOPE IT WORKS
% At what point is this useful?
We expect to see power and speed improvement by performing more <error-tolerant?> approximate computation into CNN application.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}

\end{document}



