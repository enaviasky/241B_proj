
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}

\ifCLASSINFOpdf

\else

\fi
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{High Speed Energy-Efficient Convolutional Neural Network with Approximate Computing}

\author{Keertana Settaluri,
        Emily Naviasky\\% <-this % stops a space
        Department of Electrical and Computer Engineering, UC Berkeley}

\markboth{Journal of EE241B ,~Vol.~$\pi$, No.~0, March~2016}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

\maketitle


\begin{abstract}
Approximate computing is growing in popularity due to the number of applications that require low power per computation and have loose requirements for precision. In this project we will implement an error tolerant approximate adder and approximate multiplier. Since neural nets are naturally noisy applications, the convolutional neural network application is a good algorithm to compare approximate computing to the conventional implementation. We propose to compare an accelerator for a convolutional neural net adder with an approximate version over three metrics: power, area, and speed. We will implement this in 32nm technology. We expect that the approximate version will provide a low power and faster alternative. We will also investigate the error rate which is tolerable to still get a functioning neural net.
\end{abstract}

\begin{IEEEkeywords}
Convolutional Neural Network, Approximate Computing, Energy Efficiency
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\indent Deep neural networks have gained immense popularity in the recent decade, primarily in complex learning applications such as computer vision, speech recognition, and natural language processing. However, as problems become more complex, the size of neural networks and the sheer amount of computation required drastically increases. Understandably, a significant amount of time and research is being used to develop a more efficient, less power intensive, and faster hardware implementation of neural networks. One attempt to decrease the massive increase of neurons is to change the structure of the network. For example, convolutional neural networks, which have gained popularity recently in image and signal processing applications, were developed to deal with data, such as a picture, that would otherwise require an infeasible number of neurons in a traditional neural network. However, convolutional neural networks are still extremely computationally intensive.[1] \\
	
	\indent Convolutional neural networks (convNets), in particular, are widely used in many computer vision applications such as pattern recognition and image classification. Current smart phone applications, such as Google translate [2], make use of these networks through the use of a mobile processor's SIMD instructions. In power conscious applications such as these, using specialized hardware would enable processors not necessarily built for convNets to compute faster and more energy efficient computations. Low power neural network hardware accelerators have been developed, but their speed usually comes at the expense of generalization, as in the case of Eyeriss[3]. This hardware specific optimization limits the application to only the one in which it was designed. \\	
	
	\indent ConvNets have an innate resilience to imperfections in precision due to the nature of the noisy data which they are used to interpret. Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of multiply and accumulate operations in order to speed up a system and save power. Thus, several works have investigated the use of approximate computing to trade unnecessary accuracy for power and speed in neural networks. This work seeks to investigate different implementations of approximate computing applied specifically to convNets. 

\section{Problem Description}
	\indent Using a convNet to classify a single, small image can be very computationally expensive. A typical convNet, such as AlexNet [1], uses 2.3 million weights, and requires 666 million MACs per 227x227 image. A even more intensive implementation, VGG16 [4], uses 14.7 million weights, and requires a staggering 15.3 billion MACs per 224x224 image.\\
	
%-is it worth doing approximate computing in CNN?
	\indent It is an easy decision to want to trade some of the unnecessary accuracy in such a computationally expensive algorithm for power savings and faster computation. However, accuracy for speed and power is not a decision made in a vacuum. Area on silicon is expensive and an entire block for approximate hardware would have to have significant enough power savings to be worth the area. It is useful to explore whether convNets are an appropriate application for approximate computing in a quantitative way. This paper will begin by examining convNets and approximate computing in more detail.

\subsection{Convolutional Neural Nets}
	\indent Neural networks are a classification algorithm well suited to noisy data. They are composed of a highly connected mesh of nodes, where each connection represents some weight. Each node sums up the weighted combination of the nodes from the previous layer. If the weights sum above a certain threshold function then the node is activated and contributes its weight to the next layer. An inactivate node has a weight of 0. In this way, neural nets can represent very complex functions, and the more layers, nodes, and interconnections there are, the more complex the representable functions are, but the more training is necessary. Training of neural nets is typically done using back propagation. Depending on the correctness of an output, training revisits all of the activated interconnects and negatively or positively reinforces the incorrect or correct answer by changing the mesh weights. \\
	
	\indent ConvNets use learnable weights and biases similar to regular neural networks, however the arrangement of neurons in three dimensions makes them uniquely suited for processing input images. Because each neuron in a convNet only connects to a local region of the input volume, as opposed a traditional neural network, where every neuron is connected to each other, convNets are ideal for dealing with matrix shaped data such as images.  A typical convNet architecture consists of a convolutional layer, pooling layer, and fully-connected layer. These layers compute the dot product between the weights and the local region where a neuron is located, downsampling of operations in the spatial dimension, and computing class scores, respectively. Furthermore, convNets also utilize an activation function between the convolutional and pooling layers in order to increase nonlinearity which thereby makes training faster. \\
	
	\indent ConvNets already implement approximate computing at a software level[2]. On a hardware level, most research in to power savings and approximate computation of neural networks focus on general neural nets as opposed to convNets, but have found a marked improvement in power and speed without sacrificing significant accuracy of the net. Implementations such as AxNN [6] show varying degrees of power savings for minimal loss in accuracy. AxNN uses back propagation--- an algorithm used to change weights when training neural nets--- to identify less important nodes and decrease their accuracy by shortening the bit-width used in computation. This paper claims to achieve up to 1.9X energy benefits for less than 1\% in output quality, and an even greater factor of 2.3X when 7.5\% loss is permissible at the output[6]. This conclusion validates that approximate computing can result in significant power savings. However, there are many approximate computing algorithms, besides reducing bit-width, that show even more promise for power to accuracy trade-off.

\subsection{Approximate Computing}

	\indent Hardware specific approximate computing is an optimization approach that efficiently reduces the precision of an operation in order to speed up a system and save power. Approximate computing can occur at a single operation level or at a algorithmic level. Algorithm level approximate computing has been used in applications that respond well to early termination when an answer is close enough, such as digital filters[7] and Support Vector Machines[8]. This paper implement approximate computing at an operational level.\\
	

	\subsubsection{Error Quantification}
	\indent 	There are several metrics for quantifying the error from approximate computing. Error rate (ER) is the probability of an error occurring and is common among many approximate algorithms [6]. There are several methods of calculating the error significance (ES) of the error. Error mean (EM), error mean square, hamming distance, error min/max, or some combination thereof are typical methods of characterizing ES, depending on what is most important to the application[8]. The following analysis of approximate operations will use error rate and error mean. For a neural net application, the thresholding function inside of each neuron eliminates some small constant error, but is unable to deal with infrequent but large errors. In addition, because each neuron adds up a large number of weighted inputs, a small Error mean is desirable, because even if error is frequent, the many additions will average the error and put the error close to the mean. Therefore, a higher ER is acceptable for a neural net application, but ES must be smaller. \\	
	
	\subsubsection{Approximate Adders}
	\indent Many implementations of approximate adders exist. Voltage scaling--- without simultaneously scaling frequency to ensure timing is met--- can introduce error to both adders and multipliers. Voltage scaled approximation is an effective way of decreasing power but needs to be carefully designed to ensure that significant bits behave well under scaled voltage conditions or it runs the risk of introducing very large ES.[8] In a similar vein, Speculative Latency Adders[9] allows delay to dictate error. Speculative latency adders rely on the fact that the probability of a worst case carry chain is small and will terminate early. This results in a speed increase but without much power saving, especially since this method is usually paired with error recovery.[8] The rest of this section examines adders with logic level error as opposed to transistor level error such as Error Tolerant Adders[10][11] and Lower-part-OR adders[12]. \\
	
	\indent Error Tolerant Adders divide the adder into subsections and cut the carry propagation chain between those[10]. Some of the error induced by ignoring carries is corrected in subsequent versions of the Error Tolerant Adders by taking into account just the upper bits of the previous subsection[11]. Akin to the Error Tolerant Adders, the Dithering Adder[13] cuts the carry chain, but alternates the replacement carry-in after each addition. This alternates whether the addition is an over or under approximation each time and makes the Dithering Adder good for accumulation. The error rate of Error Tolerant Adders is between 20-5\% (depending on various options) and no mention is made of EM, but the power advantage is not as good as other options with a power saving of 50\% compared to a typical adder, presumably because the answer is calculated in half the time.
	\indent The Lower-part-OR (LPO) takes removal of lower bit carries to an extreme[12]. The LPO adder defines some number of lower bits and uses OR instead of XOR. No carries are propagated in these lower bits; only a single `and` is performed on the most significant bits of the lower-part to choose a carry-in for the upper part. The paper[12] does not explore power savings, but the complete lack of carry propagation in the lower bits suggests somewhat significant power savings compared to other approximate adders that propagate carry bits through the lower bits. The interesting part of this approximate adder is that the ER is extremely high and the minimum and maximum error increase with longer lower-part. However, the EM is completely independent of the word length or the lower-part length. The probability and magnitude of error in all bits except the lowest is equal and thus the EM only -0.25. The lower power and low EM of this adder make it particularly appropriate for neural nets, and the paper[12] even test their adder on a neural net for face recognition. They found that the neural net could classify well with all of its adders approximating the lowest 9 of a 13 bit number. These characteristics make LPO a very good choice for a convNet application.\\
	  
\subsubsection{Approximate Multipliers}
	\indent Most research in approximate multipliers extends from existing approximate adder implementations. Solely replacing every partial product with any version of an approximate adder, however, creates too much error. Error correction methods or different, multiplier specific implementations have been proven to be more effective. For example, a multiplier with configurable partial recovery error detection was created by using an input pre-processing approximate adder for all partial products. This adder exchanges input bits in the case where one input bit is a 0 and the other is a 1. This stacking provides a means to cut the carry propagation chain, predicting up to a 69\% savings in power and up to 20\% decrease in delay with near negligible decrease in accuracy [14]. Compared to other approximate multipliers however, their error detection logic adds more area overhead.\\
	
	\indent Another implementation manipulates the partial product by splitting the input bits evenly into two different components. The most significant bits are left untouched, and regular partial products are calculated. Carry propagate is removed for the lower order bits, and every bit position from left to right (MSB to LSB) is checked to see if the product of any two bits is 1. If this partial product is 1, every bit after and to the right of this location is set to 1. Simulations in 0.18-$\mu$ technology estimate a minimum of 50\% savings in power, but only several cases were considered in determining this power metric [15]. Therefore, in reality this number is overly optimistic, and a significant reduction with a practical implementation is expected. \\
	
	\indent Another promising approach uses a Broken-Array Booth Multiplier and claims up to a 58\% savings in power. The multiplier works by defining a Vertical Breaking Level (VBL), where any partial products to the right of this line are not calculated. Using a world length of 12, the mean error proposed by this paper ranges from -3.50 for a VBL of three to -789 for a VBL of nine. The VBL for the 58\% power reduction was set to 15 in a 16x16 multiplication, and seems to be very reasonable [16]. 
	
\section{Implementation}
	\indent Combining convNets and approximate computing has already been shown to save power and improve speed. AxNN[6] examined approximate computing in neural net application and saw about x2 power improvement for less than 1\% quality loss. In particular AxNN noted that neural nets are a uniquely appropriate application for approximate computing. They used the fact that certain neurons in a net are less sensitive than others when determining the correct answer than others. In addition, the way that neural nets are trained naturally corrects for and decreases noise and error. AxNN also found that selection of only less sensitive nodes results in a nearly constant improvement in power per accuracy loss[6]. This suggest that we can apply approximate computing to all of the nodes regardless of their sensitivity, and apply more careful neuron selection afterwards for further improvement.
They also tested decreasing accuracy uniformly over nodes and found that power per error rate is decreased. However, the accuracy vs. power savings of both insensitive and uniform node selection decrease at a similar rate. Therefore, 
Therefore, power savings vs. error rate can be evaluated by applying approximation uniformly over neurons and the AxNN algorithmic selection of nodes would result in an improved result. [6] \\ 

	\indent Axnn saw improvements using bit-width variation to impose approximation. Bit-width approximation is a very power efficient approximation for computing, however, it introduces a very large mean error. Lower-Part-OR Adder is more power expensive but introduces only the addition of one or gate per lower bit. It's mean error, however, is a very small constant. Replacing bit-width approximation with Lower-Part-OR Adders should result in significantly higher accuracy which should allow the net to be pushed to lower power consumption for the same error rate.
Using similar inferences, we expect that the Broken-Array Booth Multiplier, which the most promising of power per accuracy compared to other multipliers, to function at lower power for equivalent error in the neural network.\\

	\indent We will implement Lower-Part-OR approximate adders as well as Broken-Array Booth multiplier in 32nm LP CMOS technology, and use Synthesis simulations to calculate average power per operation and confirm mean error. We will also implement an accurate adder and multiplier as well as a  bit-width varying approximate adder to compare performance metrics.\\

	\indent ConvNets are error-tolerant; it is difficult to judge the accuracy of them without testing data in the net and observing the change in  the accuracy with which the data is classified. We are going to use the small, practical, and well-established convolutional neural net, LeNet, which is a small architecture from the 1980s that is used to recognize handwritten digits. In addition there is a deep learning python framework from UC Berkeley already in place called Caffe. We will adapt LeNet in Caffe, artificially introducing error into neuron operations to match the approximation method from our simulated hardware. Using Caffe we will explore introducing error into different layers of the convolutional net, to see if one, two, or all of the layers are equally error tolerant. By varying the accuracy of the adders and multipliers in each neuron and testing data in the net we will develop a metric and determine the accuracy of the net as a whole.\\

	\indent Again using python, the number of adds and multiplications performed will be compiled and combined with the power numbers from the hardware simulation to obtain a approximate power measurement. \\

	\indent We expect our convNet to be able to tolerate some percentage of error in each node's arithmetic, and we are interested to see the relation between error of a neuron versus accuracy of the convNet as a whole. Despite the fact that most works do not focus on this relationship between neuron error and net accuracy, we feel that this is an interesting correlation to investigate. Which would enable generalizations and rules-of-thumb to be made about arithmetic error tolerance in many neural networks.\\

\section{Conclusion}
	\indent ConvNets are a prevalent and power intensive application, but--- based on examination of previous work--- they are also an excellent application for approximate computation. Approximate computing has been shown to improve power and speed of computation, and implementing an LPO approximate adder and Broken-Array Booth Multiplier inside of neurons should result in an improvement over those factors. We will compare power using a synopsis generated model of approximate adders and multipliers, and judge error resiliency of the approximate net using Caffe. We expect an improvement in average power vs. accuracy of the net. 

\begin{thebibliography}{1}

\bibitem{}
A. Krizhevsky, I. Sutskever, G. E. Hinton, "ImageNet Classification with Deep Convolutional Neural Networks," Neural Information Processing Systems, pp. 1097â€“1105, 2012.	
\bibitem{} Otavio Good, â€œHow Google Translate squeezes deep learning onto a phone,â€ Google Translate, July 29, 2015. http://googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html
\bibitem{} Y. Chen, T. Krishna , J. Emer, â€œEyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks,â€ ISSCC 2016.
\bibitem{} K. Simonyan, A. Zisserman, â€œVery Deep Convolutional Networks for Large- Scale Image Recognition,â€ CoRR, 2014. 
\bibitem{} CONVNET
\bibitem{} S. Venkataramani, â€œAxNN: Energy Efficient Neuromorphic System using Approximate Computing,â€ ISLPED 2014.
\bibitem{} K. Roy, A. Raghunathan, â€œApproximate Computing: An Energy-Efficient Computing Technique for Error Resilient Applications,â€ Computer Society Annual Symp. on VLSI, 2015.
\bibitem{} J. Han, M. Orshansky, â€œApproximate Computing: An Emerging Paradigm For Energy-Efficient Design,â€ ETS, 2013.
\bibitem{} S.-L. Lu, â€œSpeeding up processing with approximation circuits,â€
Computer, vol. 37, no. 3, pp. 67-73, 2004.
\bibitem{}N. Zhu, W.L. Goh and K.S. Yeo, â€œAn enhanced low-power high-speed adder for error-tolerant application,â€ in Proc. ISICâ€™09, pp. 69â€“72, 2009.
\bibitem{}N. Zhu, W.L. Goh and K.S. Yeo, â€œUltra low-power high-speed flexible probabilistic adder for error-tolerant applications,â€ in Proc. Intl. SoC Design Conf., pp. 393â€“396, 2011.
\bibitem{}H.R. Mahdiani, A. Ahmadi, S.M. Fakhraie, C. Lucas, â€œBio-inspired imprecise computational blocks for efficient vlsi implementation of soft-computing applications,â€ IEEE Trans. Circuits and Systems I: Regular
Papers, vol. 57, no. 4, pp. 850-862, April 2010.
\bibitem{}J. Miao, K. He, A. Gerstlauer and M. Orshansky â€œModeling and synthesis of quality-energy optimal approximate adders,â€ in Proc.
ICCAD, pp. 728, 2012.
\bibitem{}C. Lui, A Low-Power, "High-Performance Approximate Multiplier with Configurable Partial Error Recovery," EDAA, 2014.
\bibitem{}Kyaw, K, "Low-Power High-Speed Multiplier For Error­ Tolerant Application," EDSSC, 2010.

\bibitem{}Farshchi, F, "New Approximate Multplier for Low Power Digital Signal Processing, " IEEE, 2013.

\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Emily_Pic}}]{Emily Naviasky}
Emily Naviasky earned a Bachelors in Electrical Engineering and in Computer Science at the University of Maryland. Presently, she is a Graduate Student at UC Berkely interested in analog integrated circuits. She is upside-down, and has an innate resilience to imperfections in precision.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Keertana_Pic}}]{Keertana Settaluri}
is currently a first year doctoral student at UC Berkeley. She has gained immense popularity in the recent decade, and is small, practical, and well established. 
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


